# 用Spark进行分布式计算

1.Spark：是一个开源的分布式计算系统，基于内存计算，能在集群上快速处理大规模数据 。它采用数据并行思想，允许将大规模数据集拆分成多个小数据集并行处理，极大提升计算效率。常见应用场景包括大数据分析、机器学习、实时流处理等。
如何利用数据并行思想，Spark 是一个编程模型。

2.I/O:I/O 即 Input/Output 的缩写，指输入 / 输出 ，是计算机系统中数据传输机制，涵盖计算机与外部设备（如键盘、显示器、磁盘等 ），或计算机内部组件（如内存和 CPU ）间的数据传输。

3.仓库级计算机（Warehouse - Scale Computer）：源于集群概念，由大量服务器组成，像仓库一样大规模部署。它与超级计算机区分关键在于网络。强大且高带宽网络可简化应用程序，在大规模数据存储与处理场景（如大型数据中心）中发挥重要作用 。例如图中展示的 “Warehouse - Scale Cluster Node (Server)”，呈现了节点间通过网络、机架顶部交换机（Top - of - rack switch）等的连接架构，以及节点内组件（如 DRAM、SSD 等）间的数据传输带宽情况。

4.仓库级计算机：源于集群的概念。
将集群与超级计算机区分开的关键在于：网络。拥有一个强大且高宽带的网络，可以极大的简化应用程序。

5.消息传递模型：它是一种分布式内存通信方式，适用于没有共享内存的场景 。
以下是详细讲解：
*模型特点*

(1)无共享内存的分布式内存通信：
在这个模型里，不同的计算单元（比如线程）没有共享的内存区域 。不像在共享内存模型中，多个线程可以直接访问同一块内存来交换数据 ，这里每个线程都在自己独立的地址空间里操作 。

(2)线程在私有地址空间运行：

每个线程都有自己专属的地址空间 。就好比每个线程都有自己独立的小房间，在这个房间里它可以自由处理自己的数据 ，不用担心和其他线程的数据搞混 。

(3)通过发送 / 接收消息通信：

线程之间要交流数据，只能通过发送和接收消息来实现 。这就像人们互相不串门（没有共享内存 ），只能通过写信（发送消息 ）和收信（接收消息 ）来传递信息 。

**消息发送和接收操作**

发送（send） ：
要发送消息时，得指定接收方（recipient ），也就是告诉系统这条消息要发给哪个线程 ；还要指定要传输的缓冲区（buffer ），这里面装着实际要发送的数据 ；另外还可以选填一个消息标识符（“tag” ），用来给这条消息做个标记 ，方便接收方识别和处理 。比如你给朋友寄包裹（发送消息 ），得写清楚朋友的地址（接收方 ），包裹里装着东西（数据 ），还可以在包裹上贴个特别的标签（消息标识符 ） 。

接收（receive） ：
接收消息时，要指定发送方（sender ），也就是知道这条消息是谁发来的 ；指定存储数据的缓冲区（buffer ），用来存放接收到的数据 ；同样也可以有消息标识符 。接收操作就像是你收到一个包裹 ，得知道是谁寄来的（发送方 ），然后把包裹里的东西拿出来放到合适的地方（存储数据的缓冲区 ） 。

举例说明:

图片里有两个线程，Thread 1 和 Thread 2 ，分别在自己的地址空间里 。Thread 1 里有个变量 X ，它通过 send (X, 2, my_msg_id) 这样的操作把变量 X 的值作为消息发送给 Thread 2 ，这里 “2” 是接收方 Thread 2 ，“my_msg_id” 是消息标识符 。而 Thread 2 通过 recv (Y, 1, my_msg_id) 来接收消息 ，意思是接收来自 Thread 1（发送方 ）、带有 “my_msg_id” 标识符的消息，并把消息内容存到变量 Y 里 。

## 如何确保不丢失数据

1.构建分布式文件系统。Hadoop分布式文件系统（HDFS）：大文件分割成块，或片段。

主要访问模式：读取和追加。

------
图中介绍的是分布式文件系统（GFS ），以下是详细讲解：

### 块服务器（Chunk servers ）

- **与HDFS的关联**：块服务器其实和HDFS（Hadoop分布式文件系统 ）里的数据节点（DataNodes ）是类似的东西 。就好比在一个大仓库里，有很多小格子用来放东西，这里的块服务器就像是那些放东西的小格子 。
- **文件分块**：文件会被分割成连续的块 ，大小一般在64 - 256MB 。想象一下把一长串珠子（文件 ），按照一定长度剪成一小段一小段（文件块 ） 。
- **数据复制**：每个文件块通常会被复制2到3份 。这是为了防止某个存储位置出问题（比如硬盘坏了 ），数据就没了 。就像你把重要的文件多复印几份，分别放在不同地方 。
- **副本放置**：这些副本会尽量放在不同的机架上 。因为如果都放在一个机架上，要是这个机架出故障（比如断电 ），那数据就危险了 。不同机架就像把重要东西放在不同房间，更安全 。

### 主节点（Master node ）

- **与HDFS的关联**：主节点和HDFS里的名称节点（NameNode ）类似 。它就像一个大管家，管着整个文件系统里的信息 。
- **元数据存储**：主节点存储元数据 ，元数据就像是文件的身份证信息，记录着文件的名字、大小、存放在哪些块服务器上等信息 。而且主节点的数据通常也会被复制，这样就算主节点出问题，还有备份可以用 。

### 文件访问客户端库（Client library for file access ）

- **定位数据**：客户端库会和主节点交流，去找到文件块（数据 ）存放在哪里 。就像你要找一本书，先去问图书管理员（主节点 ）书放在哪个书架（块服务器 ） 。
- **直接连接**：知道文件块位置后，客户端库就直接连接到块服务器去读取或者写入数据 。就像你知道书在哪个书架后，直接去那个书架拿书一样。

----------

图中介绍的是Hadoop分布式文件系统（HDFS ），以下是详细讲解：

### HDFS架构

 **名称节点（Namenode ）**
  它就像整个文件系统的大管家，负责管理元数据 。元数据包含文件的名字、有多少副本、副本都存放在哪些数据节点（Datanodes ）上这些信息 。比如你去图书馆借书，图书管理员知道每本书放在哪个书架哪个位置，名称节点就类似图书管理员，知道每个文件块的存放信息 。

- **客户端（Client ）**要对文件进行操作时，会先找名称节点获取文件块的位置等元数据信息 。
- **数据节点（Datanodes ）**
- 可以把数据节点看成是一个个存储小仓库，实际的文件数据就存放在这里 。文件会被分割成一个个数据块（Blocks ），每个数据块一般大小是256MB 。
  - 为了保证数据的可靠性，每个数据块都会在多个数据节点上进行复制 。就像你把重要文件多复印几份，分别放在不同地方，这样即使一个地方出问题，数据也不会丢失 。而且这些数据节点会分布在不同的机架（Rack ）上，防止机架整体出故障导致数据丢失 。

### 客户端（Client ）

- **智能客户端**：客户端很“聪明” 。它首先会去找名称节点，询问自己想要访问的文件块都在哪些数据节点上 。就像你去图书馆找书，先问图书管理员书在哪个书架 。
- **直接访问**：在知道文件块位置后，客户端就会直接连接到对应的数据节点去读取或者写入数据 。就像你知道书在哪个书架后，直接去那个书架拿书一样 。

### 其他特点

- **全局命名空间**：整个HDFS有一个统一的命名空间 。这就好比图书馆里所有书都有一个统一的编号和分类系统，不管在哪个书架找书，都遵循这个统一规则 。在HDFS里，不管文件存放在哪个数据节点，都可以通过这个全局命名空间找到它 。
- **文件分块**：文件被分成一个个数据块存储 。这种方式便于数据的分布式存储和管理，也方便进行数据的复制和备份等操作 。

------

### Map（映射）

- **高阶函数概念**：Map是一种高阶函数，简单说就是它能把一个函数当成参数来用 。好比你有一个工具（主函数 ），这个工具能使用另一个小工具（作为参数的函数 ）来干活 。
- **操作原理**：它会把一个没有副作用的一元函数（这个函数只对输入做处理，不会影响其他外部状态 ）应用到输入序列的每个元素上，然后输出一个长度相同的新序列 。例如有个数组[3, 8, 4, 6, 3, 9, 2, 8] ，定义函数f是给每个数加10 ，那么Map操作就会把f作用到数组每个元素上，得到新数组[13, 18, 14, 16, 13, 19, 12, 18] 。
- **不同语言示例**
  - **函数式语言（以Haskell为例）**：在Haskell里，map函数的类型签名是`(a -> b) -> seq a -> seq b` ，意思是它接受一个从a类型到b类型的函数，以及一个a类型的序列，然后返回一个b类型的序列 。
    - **C++示例**：C++中通过模板类实现类似功能。先定义一个函数`f` ，功能是给输入整数加10 ，然后有数组`a`存放输入数据，`b`存放输出数据，通过`std::transform`函数来实现Map操作，把`f`应用到`a`的每个元素并输出到`b` 。

### Reduce（归约）

- **操作原理**：Reduce是把一个二元操作函数应用到每个元素和一个累加值上 。比如有个数组[3, 8, 4, 6, 3, 9, 2, 8] ，如果二元操作函数是加法，那么Reduce操作就是把数组里的数依次相加 ，先把第一个数和0相加（初始累加值 ），然后用结果加上第二个数，依次类推，最后得到总和43 。
- **函数类型定义**：在类型定义上，二元操作函数`f`的类型是`(b,a) -> b` ，表示接受两个参数（类型分别为b和a ）并返回一个b类型的值 ；`reduce`函数类型是`((b,a) -> b) -> seq a -> b` ，即接受一个二元操作函数和一个a类型的序列，最后返回一个b类型的值 。
- **Scala示例**：在Scala里定义了`reduce`函数，它接受一个函数`f`（类型是`(B, A) => B` ）和一个列表`l`（类型是`List[A]` ），然后通过函数`f`对列表元素进行归约操作并返回结果 。 